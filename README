Telco Traffic Event Processor
=============================

This SnappyData application consumes streams generated by the Telco Traffic Simulator (https://github.com/botkop/botkop-telcotraffic-simulator),
processes them and produces output for the following use cases:
- Network Congestion Monitoring
- Real-time Location Based Offerings
- Network Anomaly Detection

See also the botkop-telcotraffic-viz-node project at https://github.com/botkop/botkop-telcotraffic-viz-node for a visualization of the output streams.

Prerequisites
-------------
- a SnapyData 0.2.2 installation (see https://github.com/SnappyDataInc/snappy-poc/releases/)
- sbt or activator (see https://www.lightbend.com/community/core-tools/activator-and-sbt)
- a running kafka system (see http://kafka.apache.org/)
- the traffic simulator code (see https://github.com/botkop/botkop-telcotraffic-simulator)

Assuming kafka is set up and running, proceed with the following steps:
- configure the traffic simulator for kafka, by updating the the file conf/application.conf and setting the kafkaBroker as one of the messageBrokers.
  For example:

    messageBrokers = [ webSocketBroker, kafkaBroker ]
    kafkaBroker {
      class = "traffic.brokers.KafkaBroker"
      properties {
        bootstrap.servers = "localhost:9092"
        acks = "all"
        key.serializer = "org.apache.kafka.common.serialization.StringSerializer"
        value.serializer = "org.apache.kafka.common.serialization.StringSerializer"
      }
    }

  Next start the traffic simulator.

Building the application
------------------------
Check out the code from github and execute the following command in the source folder:
    sbt assembly
or if you are using activator:
    activator assembly

Configuring the Snappy job
--------------------------
Input, output, services, etc... are defined in the file src/main/resources/application.conf.
Please have a look.
Be ware that you will need to rebuild, if you change the contents of the file.

Submitting the Snappy job
-------------------------
Execute:
    spark-submit --master "local[*]" --class traffic.SnappyTrafficStreamProcessor target/scala-2.10/botkop-telcotraffic-snappy-assembly-1.0-SNAPSHOT.jar

In case you also have a spark (non-snappy) distribution installed, make sure your PATH has been set up correctly and you are using the spark-submit command from the snappy distribution.

The results will be published on kafka topics.
By default, the topics are:
    celltower-stats-topic
    subscriber-stats-topic
    kmeans-outlier-topic
    geofence-topic

Use case: Network Congestion Monitoring
---------------------------------------
Statistics are calculated for metrics produced on the celltower-topic by the traffic simulator.
The following base statistics are calculated in a sliding window (configurable with the parameters spark.metrics.window.size and spark.metrics.slide.size):
    count
    mean
    stdev
    max
    min
These are produced for every metric in the input stream, and grouped by celltower and subscriber.
The results are by default published on celltower-stats-topic and subscriber-stats-topic.
These can be used to generate heatmaps or to trigger alerts, for example.

Use case: Real-time Location Based Offerings
--------------------------------------------
Geofences are defined in JSON format in a file, by default work/traffic-geofences.json.
An example file has been provided.
The idea is that this file is created and updated by an external user interface.
The contents of the file will be read by the Spark job every 5 seconds.

When a subscriber connects to a celltower that is inside one of the geofences, a message will be created on the geofence-topic (by default).
This can be used for real-time location based offerings, by sending an SMS to the subscriber, for example.

Use case: Network Anomaly Detection
-----------------------------------
The metrics on the celltower-topic input stream are clustered by dimensions defined in the configuration file.
These dimensions are basically the names of the metrics you want to cluster on.
For visualization purposes, it is recommended to set the number of dimensions = 2, or perhaps 3.
Higher order visualizations are probably a bit more difficult to realize.

Currently, the number of clusters is not detected automatically, and must be defined manually in the config file.
It is best set to the number of celltower templates defined in the configuration of the traffic simulator.

The results are published on topic kmeans-outlier-topic, in a sliding window.

Spark will train a K-Means streaming model on the metrics defined by the dimensions, and predict the cluster to which each of the data points belongs.
Next the distance between the data point and the centroid of the cluster is calculated.
Based on this, the data point will be flagged as an outlier, or not.

This can be used as a basis for anomaly detection in the network.
For example, detection of celltowers belonging to a template that generates lots of metrics outside of the normal distribution, which could signify that this type of celltower is end-of-life.

